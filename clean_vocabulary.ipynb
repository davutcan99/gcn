{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Emre\n",
      "[nltk_data]     Davutcan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words extracted, stopwords filtered, and vocabulary saved to vocabulary.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Path to the mr dataset files\n",
    "data_dir = r\"C:\\Users\\Emre Davutcan\\Desktop\\preprocess\"\n",
    "comments_file = os.path.join(data_dir, \"r8_equalized_data.txt\")\n",
    "vocabulary_file = os.path.join(data_dir, \"r8_vocabulary.txt\")\n",
    "\n",
    "# Read comments\n",
    "with open(comments_file, \"r\") as file:\n",
    "    comments = file.readlines()\n",
    "\n",
    "# Extract unique words from comments\n",
    "unique_words = set()\n",
    "for comment in comments:\n",
    "    words = comment.strip().split()\n",
    "    unique_words.update(words)\n",
    "\n",
    "# Load the stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Filter stopwords from unique words\n",
    "filtered_words = [word for word in unique_words if word.lower() not in stop_words]\n",
    "\n",
    "# Save unique vocabulary to a file\n",
    "with open(vocabulary_file, \"w\") as file:\n",
    "    for word in filtered_words:\n",
    "        file.write(word + \"\\n\")\n",
    "\n",
    "print(\"Unique words extracted, stopwords filtered, and vocabulary saved to vocabulary.txt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Load data from files\n",
    "with open('mr_sentences_clean.txt', 'r') as file:\n",
    "    comments = file.read().splitlines()\n",
    "\n",
    "with open('mr_labels_clean.txt', 'r') as file:\n",
    "    labels = file.read().splitlines()\n",
    "\n",
    "# Separate sentences based on labels\n",
    "label_0_indices = [i for i, label in enumerate(labels) if label == '0']\n",
    "label_1_indices = [i for i, label in enumerate(labels) if label == '1']\n",
    "\n",
    "# Calculate the minimum count between label categories\n",
    "min_count = min(len(label_0_indices), len(label_1_indices))\n",
    "\n",
    "# Randomly select an equal number of indices from each label category\n",
    "random.seed(42)  # Set a seed for reproducibility\n",
    "selected_label_0_indices = random.sample(label_0_indices, min_count)\n",
    "selected_label_1_indices = random.sample(label_1_indices, min_count)\n",
    "\n",
    "# Combine selected indices\n",
    "selected_indices = selected_label_0_indices + selected_label_1_indices\n",
    "\n",
    "# Sort the indices to maintain the original order\n",
    "selected_indices.sort()\n",
    "\n",
    "# Select the corresponding sentences and labels\n",
    "reduced_sentences = [comments[i] for i in selected_indices]\n",
    "reduced_labels = [labels[i] for i in selected_indices]\n",
    "\n",
    "# Shuffle the combined data\n",
    "combined_data = list(zip(reduced_sentences, reduced_labels))\n",
    "random.shuffle(combined_data)\n",
    "reduced_sentences, reduced_labels = zip(*combined_data)\n",
    "\n",
    "# Select the first 2500 sentences from the shuffled data\n",
    "reduced_sentences = reduced_sentences[:2500]\n",
    "reduced_labels = reduced_labels[:2500]\n",
    "\n",
    "# Save the reduced dataset into new files\n",
    "with open('reduced_mr_comments.txt', 'w') as file:\n",
    "    file.write('\\n'.join(reduced_sentences))\n",
    "\n",
    "with open('reduced_mr_labels.txt', 'w') as file:\n",
    "    file.write('\\n'.join(reduced_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acq: 312\n",
      "crude: 312\n",
      "earn: 312\n",
      "grain: 51\n",
      "interest: 271\n",
      "money-fx: 293\n",
      "ship: 144\n",
      "trade: 312\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.read().splitlines()\n",
    "    return data\n",
    "\n",
    "def save_data(file_path, data):\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write('\\n'.join(data))\n",
    "\n",
    "def equalize_dataset(data, labels, target_size):\n",
    "    unique_labels = np.unique(labels)\n",
    "    label_counts = {label: labels.count(label) for label in unique_labels}\n",
    "\n",
    "    new_data = []\n",
    "    new_labels = []\n",
    "\n",
    "    for label in unique_labels:\n",
    "        label_data = [d for d, l in zip(data, labels) if l == label]\n",
    "        label_count = min(label_counts[label], target_size // len(unique_labels))\n",
    "        label_data = random.sample(label_data, label_count)\n",
    "        new_data.extend(label_data)\n",
    "        new_labels.extend([label] * label_count)\n",
    "\n",
    "    return new_data, new_labels\n",
    "\n",
    "# Set the file paths\n",
    "data_file = 'r8_sentences.txt'\n",
    "labels_file = 'r8_labels.txt'\n",
    "\n",
    "# Load the data and labels\n",
    "data = load_data(data_file)\n",
    "labels = load_data(labels_file)\n",
    "\n",
    "# Equalize the dataset\n",
    "target_size = 2500\n",
    "equalized_data, equalized_labels = equalize_dataset(data, labels, target_size)\n",
    "\n",
    "# Save the equalized dataset\n",
    "equalized_data_file = 'r8_equalized_data.txt'\n",
    "equalized_labels_file = 'r8_equalized_labels.txt'\n",
    "\n",
    "save_data(equalized_data_file, equalized_data)\n",
    "save_data(equalized_labels_file, equalized_labels)\n",
    "\n",
    "# Print the count of each label\n",
    "unique_labels, label_counts = np.unique(equalized_labels, return_counts=True)\n",
    "for label, count in zip(unique_labels, label_counts):\n",
    "    print(f'{label}: {count}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
